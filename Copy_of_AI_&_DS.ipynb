{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Creating, Training & Saving BERT to Google Drive\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_excel('/content/drive/MyDrive/AI and Data Science/Segment1.xlsx')  # Make sure to provide the correct path to your dataset\n",
        "\n",
        "# Encoding labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['EncodedLabels'] = label_encoder.fit_transform(df['Sentiment_3'])  # Replace 'Sentiment_3' with your actual column name\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Dataset class\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts.iloc[index]\n",
        "        label = self.labels.iloc[index]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare dataset and dataloader using all data\n",
        "dataset = EmotionDataset(df['Text'], df['EncodedLabels'], tokenizer)\n",
        "data_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Model\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch in data_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} complete.\")\n",
        "# Save the model and tokenizer to your Google Drive\n",
        "model.save_pretrained('/content/drive/MyDrive/AI and Data Science/Model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/AI and Data Science/Model')\n",
        "\n",
        "print(f\"Model and tokenizer saved to {'/content/drive/MyDrive/AI and Data Science/Model'}\")"
      ],
      "metadata": {
        "id": "OeZNIlu3WuNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSUzTI73JYQ5"
      },
      "outputs": [],
      "source": [
        "# @title Loading, Training & Saving Pre-Trained BERT to Google Drive\n",
        "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.optim import AdamW\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('/content/drive/MyDrive/AI and Data Science/Segment6.xlsx')\n",
        "label_encoder = LabelEncoder()\n",
        "df['EncodedLabels'] = label_encoder.fit_transform(df['Sentiment_3'])\n",
        "\n",
        "# Load tokenizer and model configuration\n",
        "model_path = '/content/drive/MyDrive/AI and Data Science/Model'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Check the original model configuration\n",
        "config = BertConfig.from_pretrained(model_path)\n",
        "original_num_labels = config.num_labels\n",
        "current_num_labels = len(label_encoder.classes_)\n",
        "\n",
        "# Load the model, adjust num_labels if necessary\n",
        "if original_num_labels != current_num_labels:\n",
        "    config.num_labels = current_num_labels  # Update config to new number of labels\n",
        "    model = BertForSequenceClassification(config=config)  # Reinitialize the model\n",
        "else:\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path, config=config)\n",
        "\n",
        "# Define Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare data loader\n",
        "dataset = TextDataset(df['Text'].tolist(), df['EncodedLabels'].tolist(), tokenizer)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "# Setup training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(5):\n",
        "    for batch in loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} complete.\")\n",
        "\n",
        "# Save the fine-tuned model back to Google Drive\n",
        "model.save_pretrained('/content/drive/MyDrive/AI and Data Science/Model')\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/AI and Data Science/Model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the Validation Dataset\n",
        "# Load the validation dataset\n",
        "val_df = pd.read_excel('/content/drive/MyDrive/AI and Data Science/Validation.xlsx')\n",
        "val_df['EncodedLabels'] = label_encoder.transform(val_df['Sentiment_3'])  # Assuming the same label encoder can be applied\n",
        "\n",
        "# Define the dataset class for the validation dataset\n",
        "class ValidationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "        encoding = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare the validation DataLoader\n",
        "val_dataset = ValidationDataset(val_df['Text'].tolist(), val_df['EncodedLabels'].tolist(), tokenizer)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "04YVmAF7Fi-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluating the Model and Print Results\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_model(model, data_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            predicted_labels.extend(preds.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    report = classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_)\n",
        "\n",
        "    print(f'Validation Accuracy: {accuracy}')\n",
        "    print('Classification Report:')\n",
        "    print(report)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, val_loader, device)"
      ],
      "metadata": {
        "id": "Nb-qtBZkHs4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Youtube API & Video IDs\n",
        "# Youtube API Credentials\n",
        "dev = \"AIzaSyAM07vB7FSGs46coHuu1wv3i5prszm8e54\"\n",
        "\n",
        "# List of Youtube Videos IDs\n",
        "video_ids = ['e3KCOFCI4js','1_qod_2ZIxM'# Add more video IDs here\n",
        "           ]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "B5JhsftLGaqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Function to Fetch Comments\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "api_service_name = \"youtube\"\n",
        "api_version = \"v3\"\n",
        "\n",
        "youtube = googleapiclient.discovery.build(\n",
        "    api_service_name, api_version, developerKey=dev)\n",
        "\n",
        "\n",
        "def getcomments(video):\n",
        "    request = youtube.commentThreads().list(\n",
        "        part=\"snippet\",\n",
        "        videoId=video,\n",
        "        maxResults=100\n",
        "    )\n",
        "\n",
        "    comments = []\n",
        "\n",
        "    # Executing the request.\n",
        "    response = request.execute()\n",
        "\n",
        "    # Getting the comments from the response.\n",
        "    for item in response['items']:\n",
        "        comment = item['snippet']['topLevelComment']['snippet']\n",
        "        public = item['snippet']['isPublic']\n",
        "        published_date = datetime.strptime(comment['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "        year = published_date.strftime('%Y')\n",
        "        comments.append([\n",
        "            comment['authorDisplayName'],\n",
        "            year,  # Using only the year\n",
        "            comment['likeCount'],\n",
        "            comment['textOriginal'],\n",
        "            comment['videoId'],\n",
        "            public\n",
        "        ])\n",
        "    # Fetch next pages if available\n",
        "    while True:\n",
        "        try:\n",
        "            nextPageToken = response['nextPageToken']\n",
        "        except KeyError:\n",
        "            break\n",
        "\n",
        "        # Creating a new request object with the next page token.\n",
        "        nextRequest = youtube.commentThreads().list(part=\"snippet\", videoId=video, maxResults=100, pageToken=nextPageToken)\n",
        "\n",
        "        # Executing the next request.\n",
        "        response = nextRequest.execute()\n",
        "\n",
        "        # Getting the comments from the next response.\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']\n",
        "            public = item['snippet']['isPublic']\n",
        "            published_date = datetime.strptime(comment['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "            year = published_date.strftime('%Y')\n",
        "            comments.append([\n",
        "                comment['authorDisplayName'],\n",
        "                year,  # Using only the year\n",
        "                comment['likeCount'],\n",
        "                comment['textOriginal'],\n",
        "                comment['videoId'],\n",
        "                public\n",
        "            ])\n",
        "\n",
        "    df2 = pd.DataFrame(comments, columns=['Author', 'Published Date', 'Likes', 'Text', 'Video ID', 'Public'])\n",
        "    return df2"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AFBUDxaXGfv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Calling Function to Fetch Comments\n",
        "# Initializing an empty dataframe to store comments\n",
        "df = pd.DataFrame(columns=['Author', 'Published Date', 'Likes', 'Text', 'Video ID', 'Public'])\n",
        "\n",
        "# Loop through video IDs and Fetching Comments\n",
        "for video_id in video_ids:\n",
        "    df1 = getcomments(video_id)\n",
        "    df = pd.concat([df, df1], ignore_index=True)\n",
        "\n",
        "# Displaying the DataFrame Created\n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "t2a9XU5qGg1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Saving & Downloading CSV File (Can be used whenever needed)\n",
        "from google.colab import files\n",
        "\n",
        "# Saving combined dataframe to a single CSV file\n",
        "csv_filename = \"YouTubeComments.csv\"\n",
        "df.to_csv(csv_filename, index=False)\n",
        "print(\"Combined Comments saved to:\", csv_filename)\n",
        "\n",
        "# Downloading the CSV file\n",
        "files.download(csv_filename)\n",
        "\n",
        "# Printing message indicating that the CSV file has been downloaded\n",
        "print(\"CSV file has been downloaded to your computer with the filename:\", csv_filename)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "BsUTlDi6G05_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Prediction Using Pretrained BERT\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the saved model and tokenizer\n",
        "model_path = '/content/drive/MyDrive/AI and Data Science/Model'\n",
        "\n",
        "# Load the tokenizer, model, and label encoder\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "label_encoder = joblib.load('/content/drive/MyDrive/AI and Data Science/Model/label_encoder.pkl')\n",
        "\n",
        "# Setup device for Torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def sentiment_analysis(text, model, tokenizer, label_encoder, device, max_length=512):\n",
        "    \"\"\"Perform sentiment analysis on the provided text using a pre-trained BERT model.\"\"\"\n",
        "    # Prepare the text input for BERT\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        return_token_type_ids=False,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    # Evaluate the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Process the model outputs\n",
        "    logits = outputs.logits\n",
        "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "    predicted_index = torch.argmax(probs, dim=1).cpu().numpy()[0]\n",
        "    predicted_label = label_encoder.inverse_transform([predicted_index])[0]\n",
        "    confidence = probs.cpu().numpy()[0][predicted_index]\n",
        "\n",
        "    return predicted_label, confidence\n",
        "\n",
        "# Assuming 'df' is already loaded with the necessary data\n",
        "# Load your DataFrame here if not already loaded\n",
        "# df = pd.read_excel('/content/drive/MyDrive/AI and Data Science/YourDataFrame.xlsx')\n",
        "\n",
        "# Run predictions on the 'Text' column of the DataFrame\n",
        "results = []\n",
        "for text in df['Text']:\n",
        "    predicted_label, confidence = sentiment_analysis(text, model, tokenizer, label_encoder, device)\n",
        "    results.append((text, predicted_label, confidence))\n",
        "\n",
        "# Create a DataFrame to store and display results\n",
        "result_df = pd.DataFrame(results, columns=['Text', 'Predicted Sentiment', 'Confidence'])\n",
        "\n",
        "# Optionally, save or display the DataFrame\n",
        "result_df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C1qmoVsCeuK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preprocessing\n",
        "!pip install contractions -q\n",
        "!pip install autocorrect -q\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import contractions\n",
        "from autocorrect import Speller\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Lowercasing\n",
        "df['CText'] = df['Text'].apply(lambda x: x.lower())\n",
        "\n",
        "# Handling Contractions\n",
        "df['CText'] = df['CText'].apply(lambda x: contractions.fix(x))\n",
        "\n",
        "# Removing URLs\n",
        "df['CText'] = df['CText'].apply(lambda x: re.sub(r'http\\S+|www\\S+|[^a-zA-Z\\s]', '', x))\n",
        "\n",
        "# Removing Special Characters and Emojis\n",
        "df['CText'] = df['CText'].apply(lambda x: re.sub(r'[^\\w\\s]|_+', '', x))\n",
        "\n",
        "# Removing Numbers\n",
        "df['CText'] = df['CText'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "# Whitespace Removal\n",
        "df['CText'] = df['CText'].apply(lambda x: ' '.join(x.split()))\n",
        "\n",
        "# Tokenization\n",
        "df['Tokens'] = df['CText'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# Removing Punctuation\n",
        "df['Tokens'] = df['Tokens'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
        "\n",
        "# Removing Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['Tokens'] = df['Tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['Tokens'] = df['Tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "\n",
        "# Finalizing Text by Join tokens back into a string\n",
        "df['CText'] = df['Tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Droping unnecessary columns\n",
        "df = df.drop(columns=['Author', 'Public', 'Tokens'])\n",
        "\n",
        "# Initialize the spell checker with English language\n",
        "#spell = Speller(lang='en')\n",
        "\n",
        "# Apply spell-checking to each word in the Text\n",
        "#df['CText'] = df['CText'].apply(lambda x: ' '.join([spell(word) for word in x.split()]))\n",
        "\n",
        "# Set display options to show larger text\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Display the modified DataFrame\n",
        "df"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mM-vceOQG4Ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TextBlob Function\n",
        "from textblob import TextBlob\n",
        "\n",
        "def get_sentiment_score(comment):\n",
        "    try:\n",
        "        blob = TextBlob(comment)\n",
        "        sentiment_score = blob.sentiment.polarity\n",
        "\n",
        "        # Determine sentiment category\n",
        "        if sentiment_score > 0:\n",
        "            return 'Positive'\n",
        "        elif sentiment_score < 0:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing comment: {e}\")\n",
        "        return 'Error'"
      ],
      "metadata": {
        "cellView": "form",
        "id": "brZD7LHtG-5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TB Sentiment Analysis\n",
        "# Add 'Sentiment' column to DataFrame\n",
        "df['TBSentiment2'] = df['CText'].apply(get_sentiment_score)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "pH9ef8BGSw6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TB Visualization of Sentiment Analysis\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the count of each sentiment category\n",
        "sentiment_counts = df['TBSentiment2'].value_counts()\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=100)\n",
        "plt.title('Sentiment Distribution', fontsize=20)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eLRWeBzZS1J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TB Sentiment Analysis with Like Counts\n",
        "# Add 'Sentiment' column to DataFrame\n",
        "df['TBSentiment'] = df.apply(lambda row: get_sentiment_score(row['CText']), axis=1)\n",
        "\n",
        "# Initialize counters for each sentiment category\n",
        "positive_likes = 0\n",
        "negative_likes = 0\n",
        "neutral_likes = 0\n",
        "\n",
        "# Aggregate like counts for each sentiment category\n",
        "for index, row in df.iterrows():\n",
        "    sentiment = row['TBSentiment']\n",
        "    likes = row['Likes']\n",
        "    if sentiment == 'Positive':\n",
        "        positive_likes += likes\n",
        "    elif sentiment == 'Negative':\n",
        "        negative_likes += likes\n",
        "    elif sentiment == 'Neutral':\n",
        "        neutral_likes += likes\n",
        "\n",
        "# Print the total like counts for each sentiment category\n",
        "print(\"Total Positive, Negative and Neutral Sentiments (Including Likes on the Comments)\")\n",
        "print(\"-\"*80)\n",
        "print(\"\\n\")\n",
        "print(\" \"*25,f\"Positive: {positive_likes}\")\n",
        "print(\" \"*25,f\"Negative: {negative_likes}\")\n",
        "print(\" \"*25,f\"Neutral: {neutral_likes}\")\n",
        "print(\"\\n\")\n",
        "df"
      ],
      "metadata": {
        "id": "EMrPeMPfKk_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title TB Sentiment Visualisation with Likes\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setting the figure size\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# Define counts, labels, and colors for the pie chart\n",
        "counts = [positive_likes, negative_likes, neutral_likes]\n",
        "labels = ['Positive', 'Negative', 'Neutral']\n",
        "\n",
        "# Plotting the pie chart\n",
        "patches, texts, autotexts = plt.pie(counts, labels=labels, autopct='%1.2f%%', startangle=100)\n",
        "\n",
        "# Adding count annotations to each slice\n",
        "for i, (count, autotext) in enumerate(zip(counts, autotexts)):\n",
        "    autotext.set_text(f\"{count}\\n{autotext.get_text()}\")\n",
        "\n",
        "# Setting the title of the chart\n",
        "plt.title('Sentiment Distribution', fontsize=20)\n",
        "\n",
        "# Equal aspect ratio ensures that pie is drawn as a circle\n",
        "plt.axis('equal')\n",
        "\n",
        "# Displaying the pie chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PXpzFjkgKtwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lexicon Sentiment Analysis\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Initializing SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Getting The Sentiment Scores\n",
        "df['LIntensity'] = df['CText'].apply(lambda x:sia.polarity_scores(x)['compound'])\n",
        "\n",
        "# Classifying the Sentiment scores as Positive, Negative and Neutral\n",
        "df['LSentiment'] = df['LIntensity'].apply(lambda s : 'Positive' if s > 0 else ('Neutral' if s == 0 else 'Negative'))\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "GPVcqAYgK4Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Lexixon Results Visualisation\n",
        "# Calculate the count of each sentiment category\n",
        "sentiment_counts = df['LSentiment'].value_counts()\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=100)\n",
        "plt.title('Sentiment Distribution', fontsize=20)\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QLuuRa2BK9EC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}